{"cells":[{"cell_type":"markdown","metadata":{"id":"5dd522b0"},"source":["## Chapter 3: Low-Rank Adaptation (LoRA)"],"id":"5dd522b0"},{"cell_type":"markdown","metadata":{"id":"bdd3396a"},"source":["### Spoilers\n","\n","In this chapter, we will:\n","\n","- Understand what a low-rank adapter is and why it’s useful\n","- Prepare the quantized model for training\n","- Use `peft` to create and attach adapters to a base model\n","- Discuss configuration options for targeting layers for training"],"id":"bdd3396a"},{"cell_type":"markdown","metadata":{"id":"001b25f1"},"source":["### Setup"],"id":"001b25f1"},{"cell_type":"code","execution_count":1,"metadata":{"id":"acc2f40c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762263071578,"user_tz":-330,"elapsed":23077,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}},"outputId":"efd11c2c-d375-43f0-9f70-27cdbd4f9aef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Collecting trl\n","  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n","Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n","Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.11.0)\n","Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n","Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes, trl\n","Successfully installed bitsandbytes-0.48.2 trl-0.24.0\n"]}],"source":["# If you're running on Colab\n","!pip install datasets bitsandbytes trl"],"id":"acc2f40c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ca2f769f"},"outputs":[],"source":["# If you're running on runpod.io's Jupyter Template\n","#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"],"id":"ca2f769f"},{"cell_type":"markdown","metadata":{"id":"1d2630dc"},"source":["### Imports"],"id":"1d2630dc"},{"cell_type":"code","execution_count":2,"metadata":{"id":"556455b2","executionInfo":{"status":"ok","timestamp":1762263161338,"user_tz":-330,"elapsed":47951,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from copy import deepcopy\n","from numpy.linalg import matrix_rank\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig"],"id":"556455b2"},{"cell_type":"markdown","metadata":{"id":"8836cf4f"},"source":["### The Goal\n","\n","We attach adapters to the huge linear layers in an LLM to drastically reduce the number of trainable parameters. We can easily shrink the number of trainable parameters down to less than 1% of their original number. By reducing both computation (fewer gradients to compute) and memory footprint (fewer parameters tracked by the optimizer), we achieve significant efficiency gains. Keep in mind, however, that low-rank adapters are unlikely to match the performance of full-model tuning, and their effectiveness may vary depending on the base model and the task."],"id":"8836cf4f"},{"cell_type":"markdown","metadata":{"id":"31ecd6d8"},"source":["### Pre-Reqs\n","\n","![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n","<center>Figure 3.1 - Matrix multiplication</center>"],"id":"31ecd6d8"},{"cell_type":"markdown","metadata":{"id":"461eaab6"},"source":["### Low-Rank Adaptation in a Nutshell\n","\n","![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n","<center>Figure 3.2 - Multiplying two low-rank matrices</center>"],"id":"461eaab6"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e05c5b83","outputId":"1cab2c53-01a0-4e63-b9a2-6a0f362b009d","executionInfo":{"status":"ok","timestamp":1762263252322,"user_tz":-330,"elapsed":36,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1024, 1024]), 1048576)"]},"metadata":{},"execution_count":3}],"source":["base_layer = nn.Linear(1024, 1024, bias=False)\n","base_layer.weight.shape, base_layer.weight.numel()"],"id":"e05c5b83"},{"cell_type":"markdown","metadata":{"id":"55651493"},"source":["![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n","<center>Figure 3.3 - Frozen weights and low-rank matrices</center>"],"id":"55651493"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0305a2fc","outputId":"f2ab5a44-a3aa-47d8-cbe3-fa081e16ba20","executionInfo":{"status":"ok","timestamp":1762263256740,"user_tz":-330,"elapsed":12,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Linear(in_features=1024, out_features=8, bias=False),\n"," Linear(in_features=8, out_features=1024, bias=False))"]},"metadata":{},"execution_count":4}],"source":["torch.manual_seed(11)\n","r = 8\n","layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n","layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n","layer_A, layer_B"],"id":"0305a2fc"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e716c5f3","outputId":"2217c14d-c28f-4c1d-9a5a-37f98984866a","executionInfo":{"status":"ok","timestamp":1762263259493,"user_tz":-330,"elapsed":45,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8192, 8192)"]},"metadata":{},"execution_count":5}],"source":["layer_A.weight.numel(), layer_B.weight.numel()"],"id":"e716c5f3"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da2e401c","outputId":"e194c3dc-0036-4c23-9ffd-56d6e2fc8b88","executionInfo":{"status":"ok","timestamp":1762263262057,"user_tz":-330,"elapsed":18,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1024, 1024]), 1048576)"]},"metadata":{},"execution_count":6}],"source":["composite = layer_B.weight @ layer_A.weight\n","composite.shape, composite.numel()"],"id":"da2e401c"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b25358bc","outputId":"6d71a9b8-b5d5-4736-a9d1-f5dfa12f28d9","executionInfo":{"status":"ok","timestamp":1762263266140,"user_tz":-330,"elapsed":1722,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.int64(8)"]},"metadata":{},"execution_count":7}],"source":["matrix_rank(composite.detach().numpy())"],"id":"b25358bc"},{"cell_type":"markdown","metadata":{"id":"952840a7"},"source":["$$\n","\\Large\n","\\text{output} = X @ (W + B @ A)^T\n","$$\n","<center>Equation 3.1 - Adding the resulting product to the weights</center>"],"id":"952840a7"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ea66ae2","outputId":"f132a549-0a3a-41d9-a4dc-34a2319ec2a9","executionInfo":{"status":"ok","timestamp":1762263271175,"user_tz":-330,"elapsed":41,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.4149, -1.0947,  0.9470,  ..., -1.0337,  0.0522,  0.8719]],\n","       grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":8}],"source":["torch.manual_seed(19)\n","batch = torch.randn(1, 1024)\n","\n","batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"],"id":"1ea66ae2"},{"cell_type":"markdown","metadata":{"id":"4575839d"},"source":["$$\n","\\Large\n","\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n","$$\n","<center>Equation 3.2 - Using two forward passes</center>\n","\n","![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n","<center>Figure 3.4 - Using two forward passes</center>"],"id":"4575839d"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8e9b9c3","outputId":"b5a83363-d378-4dcc-b9a7-93f9303ccbb5","executionInfo":{"status":"ok","timestamp":1762263340764,"user_tz":-330,"elapsed":45,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.3792, -1.1177,  1.4076,  ..., -0.7417, -0.1422,  0.3678]]),\n"," tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n","        grad_fn=<MmBackward0>))"]},"metadata":{},"execution_count":9}],"source":["regular_output = batch @ base_layer.weight.data.T\n","additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n","regular_output, additional_output"],"id":"a8e9b9c3"},{"cell_type":"markdown","metadata":{"id":"547f73e0"},"source":["$$\n","\\Large\n","\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n","$$\n","<center>Equation 3.3 - Chaining the adapter’s forward passes</center>"],"id":"547f73e0"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4e9cd6e","outputId":"956f3c69-ae66-4461-c1aa-a531328876d7","executionInfo":{"status":"ok","timestamp":1762263344607,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n","       grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":10}],"source":["out_A = (batch @ layer_A.weight.T)\n","additional_output = out_A @ layer_B.weight.T\n","additional_output"],"id":"f4e9cd6e"},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"267a2e77","outputId":"1e7d1f72-b51a-4ad1-f92c-f554ac0756dc","executionInfo":{"status":"ok","timestamp":1762263347213,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.3792, -1.1177,  1.4076,  ..., -0.7417, -0.1422,  0.3678]],\n","        grad_fn=<MmBackward0>),\n"," tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n","        grad_fn=<MmBackward0>),\n"," tensor([[-0.4149, -1.0947,  0.9469,  ..., -1.0337,  0.0522,  0.8719]],\n","        grad_fn=<AddBackward0>))"]},"metadata":{},"execution_count":11}],"source":["regular_output = base_layer(batch)\n","out_A = layer_A(batch)\n","additional_output = layer_B(out_A)\n","output = regular_output + additional_output\n","regular_output, additional_output, output"],"id":"267a2e77"},{"cell_type":"markdown","metadata":{"id":"9ea74713"},"source":["$$\n","\\Large\n","\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n","$$\n","<center>Equation 3.4 - LoRA’s alpha</center>"],"id":"9ea74713"},{"cell_type":"code","execution_count":12,"metadata":{"id":"uUG32xCP4qts","outputId":"8c53f30e-f476-470d-a893-253ec11e4dfe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762263351445,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.4507, -1.0717,  0.4863,  ..., -1.3257,  0.2465,  1.3760]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":12}],"source":["alpha = 2*r\n","output = regular_output + (alpha / r) * additional_output\n","output"],"id":"uUG32xCP4qts"},{"cell_type":"markdown","metadata":{"id":"55483043"},"source":["### The Road So Far"],"id":"55483043"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["ad77e1c8dc5e4b8b87f1652abeb21a68","9b74b9ac4aa84f8892c2e919b4e45540","fcd272623c154b098e3c98ea32893ffa","15c646a77aff439b9ecc2d2c5f2059e4","e91bf0fa550c431cac4410d48bfffce8","84c1be7fd76e43e59c8cc7a320e097a2","5e3da44b6fe84bfcb5152a4f480a17a5","1c28f823f6a44cefa4b75106cb4b0f04","705518c7208e4d24a1b6bccfbf0d65ff","7125688620cf4e5bb34691663ca963a3","02c6d580b52f41d3a51097bde9650472","fa118b642c354cfbb5dcf008db5a17ad","609dd97db38e432098448483aadb3929","fc1e8b4b80a343c4937f225826d3c1e5","9574a560499e4348853d8aa5dbc82478","3dcf3a4183f94a91baaf1526b62f54e8","07e716af7e1b4041b812dd18679d022f","e0ed56275f9f4d11a5ee11aa097b64cb","55c4add3e08f4790a9ac2fb64abe3872","0f4fada99c384fffa5ad3607a6dcbf57","a83b9b62faf8492cb139d702ee2a7868","d0350afc0a04450ab46f0f415f2c5484","220e96c69e684d9db68d0d16d3c0d0a9","34aaef225c52481db7b3e5ac0df3a500","576c0253c3bf4000b0e0dddb08bccc7e","1a74afb89be340deaa7a23c2358566a9","d37e95204b794cb1aedc015aad37d19c","61a5f46961224417afad6021a153f78b","49e5908fd7344fe1a0c88119d2aba93c","78c7358c96fb4ff1b38a524964ab494d","898428202375409a8195e84c85cb7285","a21d024a37ff49048d1939416cc17cc0","583837f096174ed182e28600ee68606e","0e6f5c26eb864e1d94e723bdae5d16b3","6b60736012aa48e3afcc99981070b96e","666c665c685f4b258beca97041490b3d","4e659998bc474cf3880a5ceff1f0819c","e66867f46c86498183fd0f57729703b5","299ce9e7d6ca43a0a5be7e332061ff73","0e29d18470134c73bc8e27ec6ec3bdee","3ef1468ed4524501a42d4ad3e52c4480","b078e3af60a34fb38938ad5e758e3f6c","7b39dcbfd8da4e88bb47822ab185f6ea","f5f43a1cc6d24842aee370b6cd61bd4a"]},"id":"9b713bac","outputId":"7782e806-25f2-4e21-d462-86b276fe360f","executionInfo":{"status":"ok","timestamp":1762263425779,"user_tz":-330,"elapsed":63583,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad77e1c8dc5e4b8b87f1652abeb21a68"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa118b642c354cfbb5dcf008db5a17ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220e96c69e684d9db68d0d16d3c0d0a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6f5c26eb864e1d94e723bdae5d16b3"}},"metadata":{}}],"source":["supported = torch.cuda.is_bf16_supported(including_emulation=False)\n","compute_dtype = (torch.bfloat16 if supported else torch.float32)\n","\n","nf4_config = BitsAndBytesConfig(\n","   load_in_4bit=True,\n","   bnb_4bit_quant_type=\"nf4\",\n","   bnb_4bit_use_double_quant=True,\n","   bnb_4bit_compute_dtype=compute_dtype\n",")\n","\n","model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n","                                                device_map='cpu',\n","                                                torch_dtype=compute_dtype,\n","                                                quantization_config=nf4_config)"],"id":"9b713bac"},{"cell_type":"markdown","metadata":{"id":"85a9072b"},"source":["### Parameter Types and Gradients\n","\n","****\n","**Summary of \"Parameter Types and Gradients\"**\n","- quantization only freezes the linear layers that have been quantized\n","- after quantization, a model can be prepared using the `prepare_model_for_kbit_training()` function\n","  - it freezes **all** layers\n","  - it casts every non-quantized 16-bit layer to FP32 to improve training\n","  - it enables gradient checkpointing\n","- you'll be able to unfreeze layers of your choice later on using the LoRA configuration\n","****"],"id":"85a9072b"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49a9abe7","outputId":"aa820b47-04b8-4b19-f6e8-2a77e5506ff9","scrolled":true,"executionInfo":{"status":"ok","timestamp":1762263425783,"user_tz":-330,"elapsed":2,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('decoder.embed_tokens.weight', torch.float32),\n"," ('decoder.embed_positions.weight', torch.float32),\n"," ('decoder.layers.0.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.0.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.0.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.0.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.1.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.1.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.1.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.1.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.2.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.2.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.2.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.2.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.3.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.3.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.3.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.3.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.4.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.4.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.4.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.4.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.5.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.5.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.5.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.5.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.6.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.6.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.6.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.6.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.7.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.7.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.7.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.7.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.8.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.8.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.8.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.8.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.9.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.9.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.9.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.9.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.10.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.10.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.10.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.10.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.11.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.11.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.11.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.11.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.12.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.12.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.12.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.12.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.13.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.13.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.13.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.13.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.14.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.14.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.14.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.14.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.15.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.15.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.15.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.15.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.16.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.16.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.16.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.16.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.17.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.17.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.17.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.17.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.18.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.18.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.18.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.18.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.19.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.19.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.19.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.19.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.20.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.20.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.20.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.20.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.21.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.21.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.21.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.21.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.22.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.22.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.22.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.22.final_layer_norm.bias', torch.float32),\n"," ('decoder.layers.23.self_attn_layer_norm.weight', torch.float32),\n"," ('decoder.layers.23.self_attn_layer_norm.bias', torch.float32),\n"," ('decoder.layers.23.final_layer_norm.weight', torch.float32),\n"," ('decoder.layers.23.final_layer_norm.bias', torch.float32)]"]},"metadata":{},"execution_count":14}],"source":["def trainable_parms(model):\n","    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n","    return parms\n","\n","trainable_parms(model_q4.model)"],"id":"49a9abe7"},{"cell_type":"markdown","metadata":{"id":"2132acb9"},"source":["#### `prepare_model_for_kbit_training()`"],"id":"2132acb9"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3455ebde","outputId":"ee6a6686-134c-4cd4-841b-9b66823eea50","executionInfo":{"status":"ok","timestamp":1762263483991,"user_tz":-330,"elapsed":15,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["OPTForCausalLM(\n","  (model): OPTModel(\n","    (decoder): OPTDecoder(\n","      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n","      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n","      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n","      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n","      (layers): ModuleList(\n","        (0-23): 24 x OPTDecoderLayer(\n","          (self_attn): OPTAttention(\n","            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",")"]},"metadata":{},"execution_count":15}],"source":["prepared_model = prepare_model_for_kbit_training(model_q4,\n","                                        use_gradient_checkpointing=True,\n","                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n","prepared_model"],"id":"3455ebde"},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dbad9bb","outputId":"a8decfbe-5b20-4f8d-b1a3-96a614f5cc19","scrolled":true,"executionInfo":{"status":"ok","timestamp":1762263495116,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":16}],"source":["trainable_parms(prepared_model)"],"id":"9dbad9bb"},{"cell_type":"code","execution_count":17,"metadata":{"id":"9e8be25c","executionInfo":{"status":"ok","timestamp":1762263499865,"user_tz":-330,"elapsed":39,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["def parms_of_dtype(model, dtype=torch.float32):\n","    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n","    return parms"],"id":"9e8be25c"},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"653fa73d","outputId":"e55588df-ba25-4f1a-ee6a-23e1a0e3b6c9","executionInfo":{"status":"ok","timestamp":1762263502727,"user_tz":-330,"elapsed":10,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['model.decoder.embed_tokens.weight',\n"," 'model.decoder.embed_positions.weight',\n"," 'model.decoder.layers.0.self_attn.k_proj.bias',\n"," 'model.decoder.layers.0.self_attn.v_proj.bias',\n"," 'model.decoder.layers.0.self_attn.q_proj.bias',\n"," 'model.decoder.layers.0.self_attn.out_proj.bias',\n"," 'model.decoder.layers.0.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.0.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.0.fc1.bias',\n"," 'model.decoder.layers.0.fc2.bias',\n"," 'model.decoder.layers.0.final_layer_norm.weight',\n"," 'model.decoder.layers.0.final_layer_norm.bias',\n"," 'model.decoder.layers.1.self_attn.k_proj.bias',\n"," 'model.decoder.layers.1.self_attn.v_proj.bias',\n"," 'model.decoder.layers.1.self_attn.q_proj.bias',\n"," 'model.decoder.layers.1.self_attn.out_proj.bias',\n"," 'model.decoder.layers.1.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.1.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.1.fc1.bias',\n"," 'model.decoder.layers.1.fc2.bias',\n"," 'model.decoder.layers.1.final_layer_norm.weight',\n"," 'model.decoder.layers.1.final_layer_norm.bias',\n"," 'model.decoder.layers.2.self_attn.k_proj.bias',\n"," 'model.decoder.layers.2.self_attn.v_proj.bias',\n"," 'model.decoder.layers.2.self_attn.q_proj.bias',\n"," 'model.decoder.layers.2.self_attn.out_proj.bias',\n"," 'model.decoder.layers.2.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.2.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.2.fc1.bias',\n"," 'model.decoder.layers.2.fc2.bias',\n"," 'model.decoder.layers.2.final_layer_norm.weight',\n"," 'model.decoder.layers.2.final_layer_norm.bias',\n"," 'model.decoder.layers.3.self_attn.k_proj.bias',\n"," 'model.decoder.layers.3.self_attn.v_proj.bias',\n"," 'model.decoder.layers.3.self_attn.q_proj.bias',\n"," 'model.decoder.layers.3.self_attn.out_proj.bias',\n"," 'model.decoder.layers.3.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.3.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.3.fc1.bias',\n"," 'model.decoder.layers.3.fc2.bias',\n"," 'model.decoder.layers.3.final_layer_norm.weight',\n"," 'model.decoder.layers.3.final_layer_norm.bias',\n"," 'model.decoder.layers.4.self_attn.k_proj.bias',\n"," 'model.decoder.layers.4.self_attn.v_proj.bias',\n"," 'model.decoder.layers.4.self_attn.q_proj.bias',\n"," 'model.decoder.layers.4.self_attn.out_proj.bias',\n"," 'model.decoder.layers.4.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.4.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.4.fc1.bias',\n"," 'model.decoder.layers.4.fc2.bias',\n"," 'model.decoder.layers.4.final_layer_norm.weight',\n"," 'model.decoder.layers.4.final_layer_norm.bias',\n"," 'model.decoder.layers.5.self_attn.k_proj.bias',\n"," 'model.decoder.layers.5.self_attn.v_proj.bias',\n"," 'model.decoder.layers.5.self_attn.q_proj.bias',\n"," 'model.decoder.layers.5.self_attn.out_proj.bias',\n"," 'model.decoder.layers.5.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.5.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.5.fc1.bias',\n"," 'model.decoder.layers.5.fc2.bias',\n"," 'model.decoder.layers.5.final_layer_norm.weight',\n"," 'model.decoder.layers.5.final_layer_norm.bias',\n"," 'model.decoder.layers.6.self_attn.k_proj.bias',\n"," 'model.decoder.layers.6.self_attn.v_proj.bias',\n"," 'model.decoder.layers.6.self_attn.q_proj.bias',\n"," 'model.decoder.layers.6.self_attn.out_proj.bias',\n"," 'model.decoder.layers.6.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.6.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.6.fc1.bias',\n"," 'model.decoder.layers.6.fc2.bias',\n"," 'model.decoder.layers.6.final_layer_norm.weight',\n"," 'model.decoder.layers.6.final_layer_norm.bias',\n"," 'model.decoder.layers.7.self_attn.k_proj.bias',\n"," 'model.decoder.layers.7.self_attn.v_proj.bias',\n"," 'model.decoder.layers.7.self_attn.q_proj.bias',\n"," 'model.decoder.layers.7.self_attn.out_proj.bias',\n"," 'model.decoder.layers.7.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.7.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.7.fc1.bias',\n"," 'model.decoder.layers.7.fc2.bias',\n"," 'model.decoder.layers.7.final_layer_norm.weight',\n"," 'model.decoder.layers.7.final_layer_norm.bias',\n"," 'model.decoder.layers.8.self_attn.k_proj.bias',\n"," 'model.decoder.layers.8.self_attn.v_proj.bias',\n"," 'model.decoder.layers.8.self_attn.q_proj.bias',\n"," 'model.decoder.layers.8.self_attn.out_proj.bias',\n"," 'model.decoder.layers.8.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.8.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.8.fc1.bias',\n"," 'model.decoder.layers.8.fc2.bias',\n"," 'model.decoder.layers.8.final_layer_norm.weight',\n"," 'model.decoder.layers.8.final_layer_norm.bias',\n"," 'model.decoder.layers.9.self_attn.k_proj.bias',\n"," 'model.decoder.layers.9.self_attn.v_proj.bias',\n"," 'model.decoder.layers.9.self_attn.q_proj.bias',\n"," 'model.decoder.layers.9.self_attn.out_proj.bias',\n"," 'model.decoder.layers.9.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.9.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.9.fc1.bias',\n"," 'model.decoder.layers.9.fc2.bias',\n"," 'model.decoder.layers.9.final_layer_norm.weight',\n"," 'model.decoder.layers.9.final_layer_norm.bias',\n"," 'model.decoder.layers.10.self_attn.k_proj.bias',\n"," 'model.decoder.layers.10.self_attn.v_proj.bias',\n"," 'model.decoder.layers.10.self_attn.q_proj.bias',\n"," 'model.decoder.layers.10.self_attn.out_proj.bias',\n"," 'model.decoder.layers.10.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.10.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.10.fc1.bias',\n"," 'model.decoder.layers.10.fc2.bias',\n"," 'model.decoder.layers.10.final_layer_norm.weight',\n"," 'model.decoder.layers.10.final_layer_norm.bias',\n"," 'model.decoder.layers.11.self_attn.k_proj.bias',\n"," 'model.decoder.layers.11.self_attn.v_proj.bias',\n"," 'model.decoder.layers.11.self_attn.q_proj.bias',\n"," 'model.decoder.layers.11.self_attn.out_proj.bias',\n"," 'model.decoder.layers.11.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.11.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.11.fc1.bias',\n"," 'model.decoder.layers.11.fc2.bias',\n"," 'model.decoder.layers.11.final_layer_norm.weight',\n"," 'model.decoder.layers.11.final_layer_norm.bias',\n"," 'model.decoder.layers.12.self_attn.k_proj.bias',\n"," 'model.decoder.layers.12.self_attn.v_proj.bias',\n"," 'model.decoder.layers.12.self_attn.q_proj.bias',\n"," 'model.decoder.layers.12.self_attn.out_proj.bias',\n"," 'model.decoder.layers.12.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.12.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.12.fc1.bias',\n"," 'model.decoder.layers.12.fc2.bias',\n"," 'model.decoder.layers.12.final_layer_norm.weight',\n"," 'model.decoder.layers.12.final_layer_norm.bias',\n"," 'model.decoder.layers.13.self_attn.k_proj.bias',\n"," 'model.decoder.layers.13.self_attn.v_proj.bias',\n"," 'model.decoder.layers.13.self_attn.q_proj.bias',\n"," 'model.decoder.layers.13.self_attn.out_proj.bias',\n"," 'model.decoder.layers.13.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.13.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.13.fc1.bias',\n"," 'model.decoder.layers.13.fc2.bias',\n"," 'model.decoder.layers.13.final_layer_norm.weight',\n"," 'model.decoder.layers.13.final_layer_norm.bias',\n"," 'model.decoder.layers.14.self_attn.k_proj.bias',\n"," 'model.decoder.layers.14.self_attn.v_proj.bias',\n"," 'model.decoder.layers.14.self_attn.q_proj.bias',\n"," 'model.decoder.layers.14.self_attn.out_proj.bias',\n"," 'model.decoder.layers.14.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.14.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.14.fc1.bias',\n"," 'model.decoder.layers.14.fc2.bias',\n"," 'model.decoder.layers.14.final_layer_norm.weight',\n"," 'model.decoder.layers.14.final_layer_norm.bias',\n"," 'model.decoder.layers.15.self_attn.k_proj.bias',\n"," 'model.decoder.layers.15.self_attn.v_proj.bias',\n"," 'model.decoder.layers.15.self_attn.q_proj.bias',\n"," 'model.decoder.layers.15.self_attn.out_proj.bias',\n"," 'model.decoder.layers.15.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.15.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.15.fc1.bias',\n"," 'model.decoder.layers.15.fc2.bias',\n"," 'model.decoder.layers.15.final_layer_norm.weight',\n"," 'model.decoder.layers.15.final_layer_norm.bias',\n"," 'model.decoder.layers.16.self_attn.k_proj.bias',\n"," 'model.decoder.layers.16.self_attn.v_proj.bias',\n"," 'model.decoder.layers.16.self_attn.q_proj.bias',\n"," 'model.decoder.layers.16.self_attn.out_proj.bias',\n"," 'model.decoder.layers.16.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.16.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.16.fc1.bias',\n"," 'model.decoder.layers.16.fc2.bias',\n"," 'model.decoder.layers.16.final_layer_norm.weight',\n"," 'model.decoder.layers.16.final_layer_norm.bias',\n"," 'model.decoder.layers.17.self_attn.k_proj.bias',\n"," 'model.decoder.layers.17.self_attn.v_proj.bias',\n"," 'model.decoder.layers.17.self_attn.q_proj.bias',\n"," 'model.decoder.layers.17.self_attn.out_proj.bias',\n"," 'model.decoder.layers.17.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.17.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.17.fc1.bias',\n"," 'model.decoder.layers.17.fc2.bias',\n"," 'model.decoder.layers.17.final_layer_norm.weight',\n"," 'model.decoder.layers.17.final_layer_norm.bias',\n"," 'model.decoder.layers.18.self_attn.k_proj.bias',\n"," 'model.decoder.layers.18.self_attn.v_proj.bias',\n"," 'model.decoder.layers.18.self_attn.q_proj.bias',\n"," 'model.decoder.layers.18.self_attn.out_proj.bias',\n"," 'model.decoder.layers.18.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.18.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.18.fc1.bias',\n"," 'model.decoder.layers.18.fc2.bias',\n"," 'model.decoder.layers.18.final_layer_norm.weight',\n"," 'model.decoder.layers.18.final_layer_norm.bias',\n"," 'model.decoder.layers.19.self_attn.k_proj.bias',\n"," 'model.decoder.layers.19.self_attn.v_proj.bias',\n"," 'model.decoder.layers.19.self_attn.q_proj.bias',\n"," 'model.decoder.layers.19.self_attn.out_proj.bias',\n"," 'model.decoder.layers.19.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.19.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.19.fc1.bias',\n"," 'model.decoder.layers.19.fc2.bias',\n"," 'model.decoder.layers.19.final_layer_norm.weight',\n"," 'model.decoder.layers.19.final_layer_norm.bias',\n"," 'model.decoder.layers.20.self_attn.k_proj.bias',\n"," 'model.decoder.layers.20.self_attn.v_proj.bias',\n"," 'model.decoder.layers.20.self_attn.q_proj.bias',\n"," 'model.decoder.layers.20.self_attn.out_proj.bias',\n"," 'model.decoder.layers.20.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.20.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.20.fc1.bias',\n"," 'model.decoder.layers.20.fc2.bias',\n"," 'model.decoder.layers.20.final_layer_norm.weight',\n"," 'model.decoder.layers.20.final_layer_norm.bias',\n"," 'model.decoder.layers.21.self_attn.k_proj.bias',\n"," 'model.decoder.layers.21.self_attn.v_proj.bias',\n"," 'model.decoder.layers.21.self_attn.q_proj.bias',\n"," 'model.decoder.layers.21.self_attn.out_proj.bias',\n"," 'model.decoder.layers.21.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.21.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.21.fc1.bias',\n"," 'model.decoder.layers.21.fc2.bias',\n"," 'model.decoder.layers.21.final_layer_norm.weight',\n"," 'model.decoder.layers.21.final_layer_norm.bias',\n"," 'model.decoder.layers.22.self_attn.k_proj.bias',\n"," 'model.decoder.layers.22.self_attn.v_proj.bias',\n"," 'model.decoder.layers.22.self_attn.q_proj.bias',\n"," 'model.decoder.layers.22.self_attn.out_proj.bias',\n"," 'model.decoder.layers.22.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.22.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.22.fc1.bias',\n"," 'model.decoder.layers.22.fc2.bias',\n"," 'model.decoder.layers.22.final_layer_norm.weight',\n"," 'model.decoder.layers.22.final_layer_norm.bias',\n"," 'model.decoder.layers.23.self_attn.k_proj.bias',\n"," 'model.decoder.layers.23.self_attn.v_proj.bias',\n"," 'model.decoder.layers.23.self_attn.q_proj.bias',\n"," 'model.decoder.layers.23.self_attn.out_proj.bias',\n"," 'model.decoder.layers.23.self_attn_layer_norm.weight',\n"," 'model.decoder.layers.23.self_attn_layer_norm.bias',\n"," 'model.decoder.layers.23.fc1.bias',\n"," 'model.decoder.layers.23.fc2.bias',\n"," 'model.decoder.layers.23.final_layer_norm.weight',\n"," 'model.decoder.layers.23.final_layer_norm.bias']"]},"metadata":{},"execution_count":18}],"source":["parms_of_dtype(prepared_model)"],"id":"653fa73d"},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"025520ae","outputId":"b4e623e4-e20b-411a-87f1-d0200179cba2","executionInfo":{"status":"ok","timestamp":1762263508030,"user_tz":-330,"elapsed":6,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["264.15104"]},"metadata":{},"execution_count":19}],"source":["prepared_model.get_memory_footprint()/1e6"],"id":"025520ae"},{"cell_type":"markdown","metadata":{"id":"00981764"},"source":["### PEFT\n","\n","\"_🤗 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware._\n","\n","_PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference._\"\n","\n","****\n","**Summary of \"PEFT\"**\n","- the basic configuration below should work well in many cases\n","```python\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","peft_model = get_peft_model(model, config)\n","```\n","- ranks of 8, 16, or 32 are typical, but using higher values shouldn’t significantly impact the model’s memory footprint.\n","- the scaling factor, `lora_alpha` is typically twice the rank.\n","- if your model has `Conv1D` layers, add `fan_in_fan_out=True` to your configuration\n","- if your model was recently released, you may need to specify the `target_modules` manually\n","  - typically, use the names of the massive linear layers in the attention module.\n","- by default, only the adapters are trainable\n","  - if you'd like to train other layers, such as layer norms, add them to the `modules_to_save` argument\n","  - if you're adding your own tokens to the tokenizer, you'll need to also train vocabulary-related layers such as embeddings and the model's head\n","****"],"id":"00981764"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"be9679cd","outputId":"ecb9e7ca-6cd7-4bf7-8a36-a7483d17c927","executionInfo":{"status":"ok","timestamp":1762263549026,"user_tz":-330,"elapsed":14,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)"]},"metadata":{},"execution_count":20}],"source":["lora_config = LoraConfig()\n","lora_config"],"id":"be9679cd"},{"cell_type":"code","execution_count":21,"metadata":{"id":"13211b1e","executionInfo":{"status":"ok","timestamp":1762263556593,"user_tz":-330,"elapsed":36,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"],"id":"13211b1e"},{"cell_type":"markdown","metadata":{"id":"1c79b1ea"},"source":["### `target_modules`\n","\n","Since there are new models and architectures being released on a weekly basis, chances are that there is no preconfigured list of target layers in your currently installed version of the PEFT library. In this case, you’ll be greeted with the following error:\n","\n","***\n","`ValueError: Please specify `target_modules` in `peft_config``\n","***\n","\n","Once you have the names, you can use yet another configuration argument: target_modules, which is either\n","the name or a list of the names of the modules to which you want to apply the adapters.\n","\n","**Supported Models**\n","\n","If you'd like to check if a given model's architecture is already supported by the installed version of the `peft` package, you can do the following:"],"id":"1c79b1ea"},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a3bd853","outputId":"a020a068-fee7-44e9-dcff-62de85762595","executionInfo":{"status":"ok","timestamp":1762263564679,"user_tz":-330,"elapsed":30,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'llama4', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'gemma2', 'gemma3_text', 'qwen2', 'qwen3'])"]},"metadata":{},"execution_count":22}],"source":["from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n","TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"],"id":"9a3bd853"},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a729f4bb","outputId":"4fad9fea-1791-4bb7-ff79-6ff61bbe85d9","executionInfo":{"status":"ok","timestamp":1762263567847,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['q_proj', 'v_proj', 'fc1', 'fc2']"]},"metadata":{},"execution_count":23}],"source":["TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"],"id":"a729f4bb"},{"cell_type":"markdown","metadata":{"id":"7b9a83df"},"source":["#### The PEFT Model"],"id":"7b9a83df"},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35c51eff","outputId":"a4837bb3-0186-4df3-b54c-5ac50e32e16b","executionInfo":{"status":"ok","timestamp":1762263572769,"user_tz":-330,"elapsed":86,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): OPTForCausalLM(\n","      (model): OPTModel(\n","        (decoder): OPTDecoder(\n","          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n","          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n","          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n","          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n","          (layers): ModuleList(\n","            (0-23): 24 x OPTDecoderLayer(\n","              (self_attn): OPTAttention(\n","                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","                (v_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1024, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (q_proj): lora.Linear4bit(\n","                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=1024, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=1024, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                  (lora_magnitude_vector): ModuleDict()\n","                )\n","                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","              )\n","              (activation_fn): ReLU()\n","              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n","              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n","              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","      )\n","      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":24}],"source":["peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n","peft_model"],"id":"35c51eff"},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fe2884e","outputId":"c8a94cac-e43d-4e15-849d-d434c0f82f38","executionInfo":{"status":"ok","timestamp":1762263581358,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['q_proj', 'v_proj']"]},"metadata":{},"execution_count":25}],"source":["TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"],"id":"0fe2884e"},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dc0992b0","outputId":"cc971274-c949-4fee-a2b8-8f6620a42576","executionInfo":{"status":"ok","timestamp":1762263584644,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["lora.Linear4bit(\n","  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n","  (lora_dropout): ModuleDict(\n","    (default): Dropout(p=0.05, inplace=False)\n","  )\n","  (lora_A): ModuleDict(\n","    (default): Linear(in_features=1024, out_features=8, bias=False)\n","  )\n","  (lora_B): ModuleDict(\n","    (default): Linear(in_features=8, out_features=1024, bias=False)\n","  )\n","  (lora_embedding_A): ParameterDict()\n","  (lora_embedding_B): ParameterDict()\n","  (lora_magnitude_vector): ModuleDict()\n",")"]},"metadata":{},"execution_count":26}],"source":["lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n","lin"],"id":"dc0992b0"},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77fa2a75","outputId":"809d905a-2b14-46fc-e7a5-27719b5ce220","executionInfo":{"status":"ok","timestamp":1762263587662,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"]}],"source":["peft_model.print_trainable_parameters()"],"id":"77fa2a75"},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca0a102e","outputId":"b99b5be3-6ea2-4ca0-b1f4-b6b61fb8cd2a","executionInfo":{"status":"ok","timestamp":1762263590163,"user_tz":-330,"elapsed":25,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n","  torch.float32),\n"," ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n","  torch.float32)]"]},"metadata":{},"execution_count":28}],"source":["trainable_parms(peft_model.base_model.model)"],"id":"ca0a102e"},{"cell_type":"markdown","metadata":{"id":"a9e8fefa"},"source":["#### `modules_to_save`"],"id":"a9e8fefa"},{"cell_type":"code","execution_count":29,"metadata":{"id":"7eb95263","executionInfo":{"status":"ok","timestamp":1762263596679,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=['layer_norm']\n",")"],"id":"7eb95263"},{"cell_type":"code","execution_count":30,"metadata":{"id":"57fb1553","executionInfo":{"status":"ok","timestamp":1762263599960,"user_tz":-330,"elapsed":32,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["# Since the model is modified in-place, we need to unload adapters\n","# from previous configuration to avoid mixing them.\n","# In a regular workflow, you'd load configuration only once and\n","# this wouldn't be needed.\n","_ = peft_model.unload()"],"id":"57fb1553"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9052a377","outputId":"5957e1e3-2090-4f12-9a51-b0f3f1078846","executionInfo":{"status":"ok","timestamp":1762263602517,"user_tz":-330,"elapsed":110,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 884,736 || all params: 332,081,152 || trainable%: 0.2664\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n","  warnings.warn(\n"]}],"source":["peft_model = get_peft_model(prepared_model, config)\n","peft_model.print_trainable_parameters()"],"id":"9052a377"},{"cell_type":"markdown","metadata":{"id":"a620eaf7"},"source":["#### Embeddings"],"id":"a620eaf7"},{"cell_type":"code","execution_count":32,"metadata":{"id":"13f7e0b2","executionInfo":{"status":"ok","timestamp":1762263605218,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=['layer_norm', 'embed_tokens']\n",")"],"id":"13f7e0b2"},{"cell_type":"code","execution_count":33,"metadata":{"id":"bc90eaa5","executionInfo":{"status":"ok","timestamp":1762263607816,"user_tz":-330,"elapsed":48,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["# Since the model is modified in-place, we need to unload adapters\n","# from previous configuration to avoid mixing them.\n","# In a regular workflow, you'd load configuration only once and\n","# this wouldn't be needed.\n","_ = peft_model.unload()"],"id":"bc90eaa5"},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80178211","outputId":"fa90fbee-b638-4d2f-b5b3-b7e5f0a1c718","executionInfo":{"status":"ok","timestamp":1762263611225,"user_tz":-330,"elapsed":425,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 26,624,000 || all params: 357,820,416 || trainable%: 7.4406\n"]}],"source":["peft_model = get_peft_model(prepared_model, config)\n","peft_model.print_trainable_parameters()"],"id":"80178211"},{"cell_type":"code","execution_count":35,"metadata":{"id":"04e7735c","executionInfo":{"status":"ok","timestamp":1762263617169,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",")"],"id":"04e7735c"},{"cell_type":"code","execution_count":36,"metadata":{"id":"e1d01087","executionInfo":{"status":"ok","timestamp":1762263620151,"user_tz":-330,"elapsed":42,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[],"source":["# Since the model is modified in-place, we need to unload adapters\n","# from previous configuration to avoid mixing them.\n","# In a regular workflow, you'd load configuration only once and\n","# this wouldn't be needed.\n","_ = peft_model.unload()"],"id":"e1d01087"},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a09a856","outputId":"b308c2d0-3c1a-4337-f3a4-8840baddcac6","executionInfo":{"status":"ok","timestamp":1762263622742,"user_tz":-330,"elapsed":125,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 1,192,704 || all params: 358,128,384 || trainable%: 0.3330\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n","  warnings.warn(\n"]}],"source":["peft_model = get_peft_model(prepared_model, config)\n","peft_model.print_trainable_parameters()"],"id":"3a09a856"},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f13bade5","outputId":"9997da97-2a96-44b5-f6fe-5343eb9bd04a","executionInfo":{"status":"ok","timestamp":1762263625579,"user_tz":-330,"elapsed":14,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["lora.Embedding(\n","  (base_layer): Embedding(50272, 512, padding_idx=1)\n","  (lora_dropout): ModuleDict(\n","    (default): Dropout(p=0.05, inplace=False)\n","  )\n","  (lora_A): ModuleDict()\n","  (lora_B): ModuleDict()\n","  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 8x50272])\n","  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 512x8])\n","  (lora_magnitude_vector): ModuleDict()\n",")"]},"metadata":{},"execution_count":38}],"source":["lin = peft_model.base_model.model.model.decoder.embed_tokens\n","lin"],"id":"f13bade5"},{"cell_type":"markdown","metadata":{"id":"9875a42d"},"source":["#### Managing Adapters"],"id":"9875a42d"},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":149,"referenced_widgets":["9e71ab1e98a744299cd6c523acdfaf10","7ebe5e4bf01040f1b65a06a3c5fe73dd","730fdfc5211646ae973c850f12afa62b","61ec9a651879455198b567bd1c6a82b5","f8d1cd790f9e46219aa4a71a63337e8d","fea6ed5fb7b54b148aada4c99636de3a","5904b6d4ce3047c3b7a6e8b0928a3442","eb4070f3788b4eca824f1e296b952e33","93c8603823d749ac902ae42128317684","141b629968b1485696f9a32957dee238","761d493c2b804c9c96035f3d2a14ff55","5f7d96102c0f4c9a8046083429588283","33e267008b7c41b89698f488cb3b3b70","5939040ac9804d128caeccb26dd7b955","fbe9f12faeea4e4b974410f602579fce","73d4574dd24a4cac85dc6955ee53d5ca","b93b347dc38c4d0d82f0e07d66c137cd","926533c607ea4df195186b953dc5dfc4","e01b538d5cda41b183b209ff926cbfa4","5d2d3011901f4f85a8292f0d3105132e","edb903cd563742ceb899670adeb61285","5fb6bf4922774d56bc80f5e64af5a80e"]},"id":"46154ae3","outputId":"b3001de9-4409-41a7-b500-07c81671d789","executionInfo":{"status":"ok","timestamp":1762263638298,"user_tz":-330,"elapsed":6014,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["adapter_config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e71ab1e98a744299cd6c523acdfaf10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7d96102c0f4c9a8046083429588283"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["ModuleDict(\n","  (default): Linear(in_features=1024, out_features=8, bias=False)\n","  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",")"]},"metadata":{},"execution_count":39}],"source":["peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n","lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n","lora_A"],"id":"46154ae3"},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cfaf80a","outputId":"a7d2a631-d197-4ade-9815-bcbf1217adeb","executionInfo":{"status":"ok","timestamp":1762263642832,"user_tz":-330,"elapsed":109,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["ModuleDict(\n","  (default): Linear(in_features=1024, out_features=8, bias=False)\n","  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n","  (third): Linear(in_features=1024, out_features=8, bias=False)\n",")"]},"metadata":{},"execution_count":40}],"source":["peft_model.add_adapter(adapter_name='third', peft_config=config)\n","lora_A"],"id":"9cfaf80a"},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2a9d787c","outputId":"e2fa0d96-82f2-4287-9d69-940dd8f126a0","executionInfo":{"status":"ok","timestamp":1762263645634,"user_tz":-330,"elapsed":21,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ModuleDict(\n","  (default): Linear(in_features=1024, out_features=8, bias=False)\n","  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",")"]},"metadata":{},"execution_count":41}],"source":["peft_model.delete_adapter(adapter_name='third')\n","lora_A"],"id":"2a9d787c"},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60106701","outputId":"31938864-4dc8-405a-82c6-3b9c37ac1f03","executionInfo":{"status":"ok","timestamp":1762263649077,"user_tz":-330,"elapsed":10,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['default', 'yoda'])"]},"metadata":{},"execution_count":42}],"source":["peft_model.peft_config.keys()"],"id":"60106701"},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"e69e9f7c","outputId":"3a9e8e1e-6855-4966-97bd-352d99a679ad","executionInfo":{"status":"ok","timestamp":1762263651895,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'default'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}],"source":["peft_model.active_adapter"],"id":"e69e9f7c"},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"563a806c","outputId":"86d835ba-47a0-4b2e-c3c9-304d08661a34","executionInfo":{"status":"ok","timestamp":1762263654529,"user_tz":-330,"elapsed":4,"user":{"displayName":"Akash Verma","userId":"15670392663171339427"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'yoda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}],"source":["peft_model.set_adapter('yoda')\n","peft_model.active_adapter"],"id":"563a806c"},{"cell_type":"markdown","metadata":{"id":"8c5f03c6"},"source":["```python\n","with peft_model.disable_adapter():\n","    original_outputs = peft_model(inputs)\n","\n","original_outputs = peft_model.base_model(inputs)\n","```"],"id":"8c5f03c6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8358ebf9","outputId":"b80014c8-f3dd-4068-947b-64c60dec6c0a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:424: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n","```python\n","from transformers import AutoModelForCausalLM\n","\n","# Load original tied model\n","model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n","\n","# Set the randomly initialized lm_head to the previously tied embeddings\n","model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n","\n","# Save the untied model\n","untied_model_dir = \"dir/for/untied/model\"\n","model.save_pretrained(untied_model_dir)\n","model.config.save_pretrained(untied_model_dir)\n","\n","# Now use the original model but in untied format\n","model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n","```\n","\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n","  warnings.warn(\n"]}],"source":["peft_model.merge_adapter(adapter_names=['yoda'])\n","lora_A"],"id":"8358ebf9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdea3313"},"outputs":[],"source":["peft_model.unload()\n","peft_model.base_model.model.model.decoder.layers[0].self_attn"],"id":"cdea3313"},{"cell_type":"markdown","metadata":{"id":"dda37040"},"source":["### Coming Up in \"Fine-Tuning LLMs\"\n","\n","Low-rank adapters saved the day by swooping in and enabling fast and cheap fine-tuning for LLMs. These humongous models, although powerful, are masters of a single trade—predicting the next token—thus remaining limited by the structure of their inputs. A new kind of input must be developed to enable these creatures to chat. Learn more about the incredible tale of chat templates in the next chapter of \"Fine-Tuning LLMs.\""],"id":"dda37040"}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/dvgodoy/FineTuningLLMs/blob/main/Chapter3.ipynb","timestamp":1762260099874}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ad77e1c8dc5e4b8b87f1652abeb21a68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b74b9ac4aa84f8892c2e919b4e45540","IPY_MODEL_fcd272623c154b098e3c98ea32893ffa","IPY_MODEL_15c646a77aff439b9ecc2d2c5f2059e4"],"layout":"IPY_MODEL_e91bf0fa550c431cac4410d48bfffce8"}},"9b74b9ac4aa84f8892c2e919b4e45540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84c1be7fd76e43e59c8cc7a320e097a2","placeholder":"​","style":"IPY_MODEL_5e3da44b6fe84bfcb5152a4f480a17a5","value":"config.json: 100%"}},"fcd272623c154b098e3c98ea32893ffa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c28f823f6a44cefa4b75106cb4b0f04","max":644,"min":0,"orientation":"horizontal","style":"IPY_MODEL_705518c7208e4d24a1b6bccfbf0d65ff","value":644}},"15c646a77aff439b9ecc2d2c5f2059e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7125688620cf4e5bb34691663ca963a3","placeholder":"​","style":"IPY_MODEL_02c6d580b52f41d3a51097bde9650472","value":" 644/644 [00:00&lt;00:00, 45.7kB/s]"}},"e91bf0fa550c431cac4410d48bfffce8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84c1be7fd76e43e59c8cc7a320e097a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3da44b6fe84bfcb5152a4f480a17a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c28f823f6a44cefa4b75106cb4b0f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"705518c7208e4d24a1b6bccfbf0d65ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7125688620cf4e5bb34691663ca963a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02c6d580b52f41d3a51097bde9650472":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa118b642c354cfbb5dcf008db5a17ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_609dd97db38e432098448483aadb3929","IPY_MODEL_fc1e8b4b80a343c4937f225826d3c1e5","IPY_MODEL_9574a560499e4348853d8aa5dbc82478"],"layout":"IPY_MODEL_3dcf3a4183f94a91baaf1526b62f54e8"}},"609dd97db38e432098448483aadb3929":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07e716af7e1b4041b812dd18679d022f","placeholder":"​","style":"IPY_MODEL_e0ed56275f9f4d11a5ee11aa097b64cb","value":"pytorch_model.bin: 100%"}},"fc1e8b4b80a343c4937f225826d3c1e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55c4add3e08f4790a9ac2fb64abe3872","max":662513657,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f4fada99c384fffa5ad3607a6dcbf57","value":662513657}},"9574a560499e4348853d8aa5dbc82478":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a83b9b62faf8492cb139d702ee2a7868","placeholder":"​","style":"IPY_MODEL_d0350afc0a04450ab46f0f415f2c5484","value":" 663M/663M [00:05&lt;00:00, 253MB/s]"}},"3dcf3a4183f94a91baaf1526b62f54e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07e716af7e1b4041b812dd18679d022f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ed56275f9f4d11a5ee11aa097b64cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55c4add3e08f4790a9ac2fb64abe3872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4fada99c384fffa5ad3607a6dcbf57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a83b9b62faf8492cb139d702ee2a7868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0350afc0a04450ab46f0f415f2c5484":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"220e96c69e684d9db68d0d16d3c0d0a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34aaef225c52481db7b3e5ac0df3a500","IPY_MODEL_576c0253c3bf4000b0e0dddb08bccc7e","IPY_MODEL_1a74afb89be340deaa7a23c2358566a9"],"layout":"IPY_MODEL_d37e95204b794cb1aedc015aad37d19c"}},"34aaef225c52481db7b3e5ac0df3a500":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61a5f46961224417afad6021a153f78b","placeholder":"​","style":"IPY_MODEL_49e5908fd7344fe1a0c88119d2aba93c","value":"model.safetensors: 100%"}},"576c0253c3bf4000b0e0dddb08bccc7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78c7358c96fb4ff1b38a524964ab494d","max":662435448,"min":0,"orientation":"horizontal","style":"IPY_MODEL_898428202375409a8195e84c85cb7285","value":662435448}},"1a74afb89be340deaa7a23c2358566a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a21d024a37ff49048d1939416cc17cc0","placeholder":"​","style":"IPY_MODEL_583837f096174ed182e28600ee68606e","value":" 662M/662M [00:07&lt;00:00, 65.0MB/s]"}},"d37e95204b794cb1aedc015aad37d19c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61a5f46961224417afad6021a153f78b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49e5908fd7344fe1a0c88119d2aba93c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78c7358c96fb4ff1b38a524964ab494d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"898428202375409a8195e84c85cb7285":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a21d024a37ff49048d1939416cc17cc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"583837f096174ed182e28600ee68606e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e6f5c26eb864e1d94e723bdae5d16b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b60736012aa48e3afcc99981070b96e","IPY_MODEL_666c665c685f4b258beca97041490b3d","IPY_MODEL_4e659998bc474cf3880a5ceff1f0819c"],"layout":"IPY_MODEL_e66867f46c86498183fd0f57729703b5"}},"6b60736012aa48e3afcc99981070b96e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_299ce9e7d6ca43a0a5be7e332061ff73","placeholder":"​","style":"IPY_MODEL_0e29d18470134c73bc8e27ec6ec3bdee","value":"generation_config.json: 100%"}},"666c665c685f4b258beca97041490b3d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ef1468ed4524501a42d4ad3e52c4480","max":137,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b078e3af60a34fb38938ad5e758e3f6c","value":137}},"4e659998bc474cf3880a5ceff1f0819c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b39dcbfd8da4e88bb47822ab185f6ea","placeholder":"​","style":"IPY_MODEL_f5f43a1cc6d24842aee370b6cd61bd4a","value":" 137/137 [00:00&lt;00:00, 4.66kB/s]"}},"e66867f46c86498183fd0f57729703b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"299ce9e7d6ca43a0a5be7e332061ff73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e29d18470134c73bc8e27ec6ec3bdee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef1468ed4524501a42d4ad3e52c4480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b078e3af60a34fb38938ad5e758e3f6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b39dcbfd8da4e88bb47822ab185f6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5f43a1cc6d24842aee370b6cd61bd4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e71ab1e98a744299cd6c523acdfaf10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ebe5e4bf01040f1b65a06a3c5fe73dd","IPY_MODEL_730fdfc5211646ae973c850f12afa62b","IPY_MODEL_61ec9a651879455198b567bd1c6a82b5"],"layout":"IPY_MODEL_f8d1cd790f9e46219aa4a71a63337e8d"}},"7ebe5e4bf01040f1b65a06a3c5fe73dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fea6ed5fb7b54b148aada4c99636de3a","placeholder":"​","style":"IPY_MODEL_5904b6d4ce3047c3b7a6e8b0928a3442","value":"adapter_config.json: 100%"}},"730fdfc5211646ae973c850f12afa62b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb4070f3788b4eca824f1e296b952e33","max":642,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93c8603823d749ac902ae42128317684","value":642}},"61ec9a651879455198b567bd1c6a82b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_141b629968b1485696f9a32957dee238","placeholder":"​","style":"IPY_MODEL_761d493c2b804c9c96035f3d2a14ff55","value":" 642/642 [00:00&lt;00:00, 49.4kB/s]"}},"f8d1cd790f9e46219aa4a71a63337e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fea6ed5fb7b54b148aada4c99636de3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5904b6d4ce3047c3b7a6e8b0928a3442":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb4070f3788b4eca824f1e296b952e33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c8603823d749ac902ae42128317684":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"141b629968b1485696f9a32957dee238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"761d493c2b804c9c96035f3d2a14ff55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f7d96102c0f4c9a8046083429588283":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33e267008b7c41b89698f488cb3b3b70","IPY_MODEL_5939040ac9804d128caeccb26dd7b955","IPY_MODEL_fbe9f12faeea4e4b974410f602579fce"],"layout":"IPY_MODEL_73d4574dd24a4cac85dc6955ee53d5ca"}},"33e267008b7c41b89698f488cb3b3b70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b93b347dc38c4d0d82f0e07d66c137cd","placeholder":"​","style":"IPY_MODEL_926533c607ea4df195186b953dc5dfc4","value":"adapter_model.safetensors: 100%"}},"5939040ac9804d128caeccb26dd7b955":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01b538d5cda41b183b209ff926cbfa4","max":3159096,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d2d3011901f4f85a8292f0d3105132e","value":3159096}},"fbe9f12faeea4e4b974410f602579fce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edb903cd563742ceb899670adeb61285","placeholder":"​","style":"IPY_MODEL_5fb6bf4922774d56bc80f5e64af5a80e","value":" 3.16M/3.16M [00:03&lt;00:00, 820kB/s]"}},"73d4574dd24a4cac85dc6955ee53d5ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b93b347dc38c4d0d82f0e07d66c137cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926533c607ea4df195186b953dc5dfc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e01b538d5cda41b183b209ff926cbfa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d2d3011901f4f85a8292f0d3105132e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edb903cd563742ceb899670adeb61285":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fb6bf4922774d56bc80f5e64af5a80e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}